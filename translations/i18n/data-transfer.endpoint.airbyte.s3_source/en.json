{
    "endpoint.airbyte.s3_source.S3Source.Csv.additional_reader_options.title": "Additional reader options",
    "endpoint.airbyte.s3_source.S3Source.Csv.additional_reader_options.usage": "Optionally add a valid JSON string to provide additional options to the CSV reader. Mappings must correspond to options <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions\" target=\"_blank\">detailed here</a>. column_types is used internally to handle schema so overriding that would likely cause problems. Example: {\"timestamp_parsers\": [\"%m/%d/%Y %H:%M\", \"%Y/%m/%d %H:%M\"], \"strings_can_be_null\": true, \"null_values\": [\"NA\", \"NULL\"]}",
    "endpoint.airbyte.s3_source.S3Source.Csv.advanced_options.title": "Advanced options",
    "endpoint.airbyte.s3_source.S3Source.Csv.advanced_options.usage": "Optionally add a valid JSON string here to provide additional <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions\" target=\"_blank\">Pyarrow ReadOptions</a>. Specify column_names here if your CSV doesnt have header, or if you want to use custom column names.",
    "endpoint.airbyte.s3_source.S3Source.Csv.block_size.title": "Block size",
    "endpoint.airbyte.s3_source.S3Source.Csv.block_size.usage": "The chunk size in bytes to process at a time in memory from each file. If your data is particularly wide and failing during schema detection, increasing this should solve it. Beware of raising this too high as you could hit OOM errors.",
    "endpoint.airbyte.s3_source.S3Source.Csv.delimiter.title": "Delimiter",
    "endpoint.airbyte.s3_source.S3Source.Csv.delimiter.usage": "The character delimiting individual cells in the CSV data. This may only be a 1-character string.",
    "endpoint.airbyte.s3_source.S3Source.Csv.double_quote.title": "Double quote",
    "endpoint.airbyte.s3_source.S3Source.Csv.double_quote.usage": "Whether two quotes in a quoted CSV value denote a single quote in the data",
    "endpoint.airbyte.s3_source.S3Source.Csv.encoding.title": "Encoding",
    "endpoint.airbyte.s3_source.S3Source.Csv.encoding.usage": "The character encoding of the CSV data. Leave blank to default to UTF-8. See <a href=\"https://docs.python.org/3/library/codecs.html#standard-encodings\" target=\"_blank\">list of python encodings</a> for available options.",
    "endpoint.airbyte.s3_source.S3Source.Csv.escape_char.title": "Escape char",
    "endpoint.airbyte.s3_source.S3Source.Csv.escape_char.usage": "The character used optionally for escaping special characters. To disallow escaping, leave this field blank.",
    "endpoint.airbyte.s3_source.S3Source.Csv.newlines_in_values.title": "Newlines in values",
    "endpoint.airbyte.s3_source.S3Source.Csv.newlines_in_values.usage": "Whether newline characters are allowed in CSV values. Turning this on may affect performance. Leave blank to default to False.",
    "endpoint.airbyte.s3_source.S3Source.Csv.quote_char.title": "Quote char",
    "endpoint.airbyte.s3_source.S3Source.Csv.quote_char.usage": "The character used optionally for quoting CSV values. To disallow quoting, make this field blank.",
    "endpoint.airbyte.s3_source.S3Source.Format.avro.title": "Avro",
    "endpoint.airbyte.s3_source.S3Source.Format.avro.usage": "This connector utilises <a href=\"https://fastavro.readthedocs.io/en/latest/\" target=\"_blank\">fastavro</a> for Avro parsing.",
    "endpoint.airbyte.s3_source.S3Source.Format.csv.title": "CSV",
    "endpoint.airbyte.s3_source.S3Source.Format.csv.usage": "This connector utilises <a href=\"https: // arrow.apache.org/docs/python/generated/pyarrow.csv.open_csv.html\" target=\"_blank\">PyArrow (Apache Arrow)</a> for CSV parsing.",
    "endpoint.airbyte.s3_source.S3Source.Format.jsonl.title": "JSON Lines",
    "endpoint.airbyte.s3_source.S3Source.Format.jsonl.usage": "This connector uses <a href=\"https://arrow.apache.org/docs/python/json.html\" target=\"_blank\">PyArrow</a> for JSON Lines (jsonl) file parsing.",
    "endpoint.airbyte.s3_source.S3Source.Format.parquet.title": "parquet",
    "endpoint.airbyte.s3_source.S3Source.Format.parquet.usage": "This connector utilises <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html\" target=\"_blank\">PyArrow (Apache Arrow)</a> for Parquet parsing.",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.UnexpectedFieldBehavior.UNEXPECTED_FIELD_BEHAVIOR_ERROR.title": "Error",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.UnexpectedFieldBehavior.UNEXPECTED_FIELD_BEHAVIOR_IGNORE.title": "Ignore",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.UnexpectedFieldBehavior.UNEXPECTED_FIELD_BEHAVIOR_INFER.title": "Infer",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.block_size.title": "Block Size",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.block_size.usage": "The chunk size in bytes to process at a time in memory from each file. If your data is particularly wide and failing during schema detection, increasing this should solve it. Beware of raising this too high as you could hit OOM errors.",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.newlines_in_values.title": "Allow newlines in values",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.newlines_in_values.usage": "Whether newline characters are allowed in JSON values. Turning this on may affect performance. Leave blank to default to False.",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.unexpected_field_behavior.title": "Unexpected field behavior",
    "endpoint.airbyte.s3_source.S3Source.Jsonl.unexpected_field_behavior.usage": "How JSON fields outside of explicit_schema (if given) are treated. Check <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.json.ParseOptions.html\" target=\"_blank\">PyArrow documentation</a> for details",
    "endpoint.airbyte.s3_source.S3Source.Parquet.batch_size.title": "Batch size",
    "endpoint.airbyte.s3_source.S3Source.Parquet.batch_size.usage": "Maximum number of records per batch. Batches may be smaller if there are not enough rows in the file. This option can help to optimize a work with memory if your data is particularly wide or failing during detection of OOM errors.",
    "endpoint.airbyte.s3_source.S3Source.Parquet.buffer_size.title": "Buffer size",
    "endpoint.airbyte.s3_source.S3Source.Parquet.buffer_size.usage": "Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help to optimize a work with memory if your data is particularly wide or failing during detection of OOM errors.",
    "endpoint.airbyte.s3_source.S3Source.Parquet.columns.title": "Columns",
    "endpoint.airbyte.s3_source.S3Source.Parquet.columns.usage": "Specify the columns from the files you want to sync. Leave it blank to keep all columns in sync.",
    "endpoint.airbyte.s3_source.S3Source.Provider.aws_access_key_id.title": "Access Key ID",
    "endpoint.airbyte.s3_source.S3Source.Provider.aws_access_key_id.usage": "In order to access private Buckets, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not necessary.",
    "endpoint.airbyte.s3_source.S3Source.Provider.aws_secret_access_key.title": "Secret Access Key",
    "endpoint.airbyte.s3_source.S3Source.Provider.aws_secret_access_key.usage": "In order to access private Buckets, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not necessary.",
    "endpoint.airbyte.s3_source.S3Source.Provider.bucket.title": "Bucket",
    "endpoint.airbyte.s3_source.S3Source.Provider.bucket.usage": "Name of the S3 bucket where the files exist.",
    "endpoint.airbyte.s3_source.S3Source.Provider.endpoint.title": "Endpoint",
    "endpoint.airbyte.s3_source.S3Source.Provider.endpoint.usage": "Endpoint to an S3 compatible service. Leave empty to use AWS.",
    "endpoint.airbyte.s3_source.S3Source.Provider.path_prefix.title": "Path prefix",
    "endpoint.airbyte.s3_source.S3Source.Provider.path_prefix.usage": "By providing a path-like prefix (for example, myFolder or thisTable/) under which all the relevant files sit, we can optimise finding these in S3. This is optional but recommended if your bucket contains many folders and files.",
    "endpoint.airbyte.s3_source.S3Source.Provider.use_ssl.title": "Use SSL",
    "endpoint.airbyte.s3_source.S3Source.Provider.use_ssl.usage": "Is remote server using secure SSL/TLS connection",
    "endpoint.airbyte.s3_source.S3Source.Provider.verify_ssl_cert.title": "Verify SSL certificate",
    "endpoint.airbyte.s3_source.S3Source.Provider.verify_ssl_cert.usage": "Allow self-signed certificates",
    "endpoint.airbyte.s3_source.S3Source.dataset.title": "Dataset",
    "endpoint.airbyte.s3_source.S3Source.dataset.usage": "This source creates one table per connection, this field is the name of that table. This should include only letters, numbers, dash and underscores. Note that this may be altered according to destination.",
    "endpoint.airbyte.s3_source.S3Source.path_pattern.title": "Path pattern",
    "endpoint.airbyte.s3_source.S3Source.path_pattern.usage": "Add at least one pattern here to match filepaths against. Use | to separate multiple patterns. Airbyte uses these patterns to determine which files to pick up from the provider storage. See <a href=\"https://facelessuser.github.io/wcmatch/glob/\" target=\"_blank\">wcmatch.glob</a> to understand pattern syntax (GLOBSTAR and SPLIT flags are enabled). Use pattern <strong>**</strong> to pick up all files. Examples: \"**\" \"myFolder/myTableFiles/*.csv|myFolder/myOtherTableFiles/*.csv\"",
    "endpoint.airbyte.s3_source.S3Source.provider.title": "S3: Amazon Web Services",
    "endpoint.airbyte.s3_source.S3Source.schema.title": "Schema",
    "endpoint.airbyte.s3_source.S3Source.schema.usage": "Optionally provide a schema to enforce, as a valid JSON string. Ensure this is a mapping of <strong>{ \"column\" : \"type\" }</strong>, where types are valid <a href=\"https://json-schema.org/understanding-json-schema/reference/type.html\" target=\"_blank\">JSON Schema datatypes</a>. Leave as {} to auto-infer the schema. Example: {\"column_1\": \"number\", \"column_2\": \"string\", \"column_3\": \"array\", \"column_4\": \"object\", \"column_5\": \"boolean\"}",
    "endpoint.airbyte.s3_source.S3Source.title": "Endpoint parameters"
}
